{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "BuRPh3sDehIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_training_data(path):\n",
        "    with open(path, 'r') as file:\n",
        "        line = file.readline().strip()\n",
        "        line_counter = 0\n",
        "        while line:\n",
        "            line_counter += 1\n",
        "\n",
        "            if line_counter == 2: # the line contains the values of P, N, M\n",
        "                line = line[1:] # skip the # sign at the beginning of the line\n",
        "                line_segments = line.split(' ')\n",
        "                for segment in line_segments:\n",
        "                    segment = segment.strip()\n",
        "                    if segment != '':\n",
        "                        if segment[0] == 'P': # P=<P_VALUE>\n",
        "                            P = int(segment[2:])\n",
        "                        elif segment[0] == 'N': # N=<N_VALUE>\n",
        "                            N = int(segment[2:])\n",
        "                        elif segment[0] == 'M': # M=<M_VALUE>\n",
        "                            M = int(segment[2:])\n",
        "                \n",
        "                X_train = np.empty((P, N))\n",
        "                Y_train = np.empty((P, M))\n",
        "            \n",
        "            elif line_counter >= 3: # contains training data as well as desired output starting at line 3\n",
        "                # line_segments would be smth. like:\n",
        "                # ['1.0', '1.0', '1.0', '', '0.9']\n",
        "                # where the first N items are the values\n",
        "                # for input data dimensions and the\n",
        "                # last M items are the last layer's\n",
        "                # outputs\n",
        "\n",
        "                line_segments = line.split(' ')\n",
        "                X_train[line_counter-3] = np.asarray(list(map(float, line_segments[:N])))\n",
        "                Y_train[line_counter-3] = np.asarray(list(map(float, line_segments[-M:])))\n",
        "                \n",
        "            line = file.readline().strip()\n",
        "    \n",
        "    return P, N, M, X_train, Y_train"
      ],
      "metadata": {
        "id": "oJXMGmhkqePG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_function(z):\n",
        "    return 1/(1+np.exp(-z))"
      ],
      "metadata": {
        "id": "YrUFoHyykDjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_first_derivative(z):\n",
        "    f = logistic_function(z)\n",
        "    return f*(1-f)"
      ],
      "metadata": {
        "id": "0nlg68h8096a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(P, Y_train, predictions):\n",
        "    mean_squared_error = 0\n",
        "    for p in range(P): # iterate over all samples\n",
        "        mean_squared_error += ((Y_train[p]-predictions[p])**2) / P\n",
        "    \n",
        "    return mean_squared_error"
      ],
      "metadata": {
        "id": "oMQBFR8sizRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptron implementation"
      ],
      "metadata": {
        "id": "ybaLY4Pe0bsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_squared_errors_different_data = []"
      ],
      "metadata": {
        "id": "602OFgyTz8Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "data_path:\n",
        "             the path to one of files\n",
        "            containing training data\n",
        "\n",
        "P: \n",
        "             the number of training \n",
        "            samples the input data contain\n",
        "\n",
        "N:\n",
        "             the dimension of the\n",
        "            input data\n",
        "\n",
        "M:\n",
        "             the dimension of the \n",
        "            network's output (i.e. the # of\n",
        "            output layer's neurons)\n",
        "\n",
        "X_train:\n",
        "            is a matrix of shape (P, N), \n",
        "            containing the input data features\n",
        "\n",
        "Y_train:\n",
        "            is a matrix of shape (P, M),\n",
        "            contating the desired output data\n",
        "            (ground truth)\n",
        "\n",
        "w_nm[N][M]:   \n",
        "             the weight going from Nth neuron\n",
        "            of layer n to Mth neuron of layer m\n",
        "\n",
        "out_n[N]:\n",
        "             the output of Nth neuron of layer n\n",
        "\n",
        "net_m[M]:\n",
        "            the output of the Mth neuron of the layer m (output layer)\n",
        "'''\n",
        "\n",
        "data_path = './PA-A_training_data_06.txt'\n",
        "P, N, M, X_train, Y_train = parse_training_data(data_path)\n",
        "\n",
        "w_nm = np.random.rand(N+1, M) #range [0, 1]\n",
        "w_nm = w_nm - 0.5 #range [-0.5, 0.5]\n",
        "\n",
        "out_n = np.empty((P, N+1))\n",
        "out_n[:,0] = 1\n",
        "out_n[:,1:] = X_train\n",
        "\n",
        "net_m = np.random.randn(M)"
      ],
      "metadata": {
        "id": "_YE6u1boC-UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "!!! Please note that, in order not to have to consider BIAS-WEIGHT separately, we consider it as a conventional weight, located at index 0. That's why in the above cell, the first dim is of size *(N+1)* and *out_n[0] = 1*"
      ],
      "metadata": {
        "id": "JmzLj3brl2O-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptron learning (delta rule)"
      ],
      "metadata": {
        "id": "RbbYIWoV0ggb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(p, out_n, w_nm):\n",
        "    '''\n",
        "    Performs forward-pass for a single train sample\n",
        "\n",
        "    p:  the sample number, we tend to perform\n",
        "        forward-pass for\n",
        "\n",
        "    out_n:  the output of previous layer\n",
        "            shape: (P, N)\n",
        "    \n",
        "    w_nm:  the weights of neurons from previous\n",
        "        layer connecting to this layer\n",
        "        shape: (N, M)\n",
        "    '''\n",
        "    \n",
        "    output = np.empty(w_nm.shape[1])\n",
        "    sum = np.zeros(w_nm.shape[1])\n",
        "    for m in range(M): # iterate over output layer's neurons\n",
        "        for n in range(w_nm.shape[0]): # iterate over the weights between input n and output m\n",
        "            sum[m] += out_n[p][n]*w_nm[n][m]\n",
        "        \n",
        "        output[m] = logistic_function(sum[m])\n",
        "    \n",
        "    return sum, output"
      ],
      "metadata": {
        "id": "VAakv3J4ruOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_weights(w_nm, delta):\n",
        "    '''\n",
        "    Update the weights based on delta rule\n",
        "    '''\n",
        "    w_nm_new = w_nm + delta\n",
        "    return w_nm_new"
      ],
      "metadata": {
        "id": "nEfMkLRZwQFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(p, N, M, net_m, out_m, out_n, Y_train, w_nm, eta):\n",
        "    '''\n",
        "    Performs backward-pass for a single train sample\n",
        "\n",
        "    p:  the sample number, we tend to perform\n",
        "        backward-pass for\n",
        "\n",
        "    N:  the total number of last layer's neurons\n",
        "\n",
        "    M:  the total number of this layer's neurons\n",
        "        (i.e. number of output neurons for this layer)\n",
        "\n",
        "    net_m:  the output of layer m before applying\n",
        "        transfer function (i.e. weighted sum)\n",
        "\n",
        "    out_m:  the output of layer m after applying\n",
        "        transfer function (i.e. f(net_m))\n",
        "\n",
        "    out_n:  the output of previous layer\n",
        "            shape: (P, N)\n",
        "\n",
        "    Y_train: contatins the ground truth\n",
        "    \n",
        "    w_nm:  the weights of neurons from previous\n",
        "        layer connecting to this layer\n",
        "        shape: (N, M)\n",
        "\n",
        "    eta: learning rate (step size)\n",
        "    '''\n",
        "    delta = np.empty(M)\n",
        "    delta_w_nm = np.empty((N+1,M))\n",
        "    for n in range(N+1):\n",
        "        for m in range(M):\n",
        "            delta[m] = (Y_train[p][m]-out_m[m])*logistic_first_derivative(net_m[m])\n",
        "            delta_w_nm[n][m] = eta * delta[m] * out_n[p][n]\n",
        "            w_nm[n][m] = update_weights(w_nm[n][m], delta_w_nm[n][m])"
      ],
      "metadata": {
        "id": "QBw8VhpAuK3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.5 # learning rate\n",
        "EPOCHS = 200"
      ],
      "metadata": {
        "id": "lweI1xFG76JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_squared_errors_list = [] # used to track of MSE in each iteration for further plotting\n",
        "for epoch in range(EPOCHS):\n",
        "    predictions = np.empty((P, M)) # store the output of the model for whole samples, so this can be used further to evaluate the model\n",
        "    for p in range(P): # iterate over training samples\n",
        "\n",
        "        net_m, out_m = forward_pass(p, out_n, w_nm)\n",
        "        predictions[p] = out_m\n",
        "        backward_pass(p, N, M, net_m, out_m, out_n, Y_train, w_nm, eta)\n",
        "\n",
        "    mean_squared_error = MSE(P, Y_train, predictions)\n",
        "    mean_squared_error_average = np.mean(mean_squared_error)\n",
        "    mean_squared_errors_list.append(mean_squared_error_average)\n",
        "\n",
        "    if epoch % int(EPOCHS/10) == 0:\n",
        "        print(f'Epoch:{epoch}, MSE={mean_squared_error}, avg: {mean_squared_error_average:.4f}')\n",
        "\n",
        "mean_squared_errors_different_data.append({\n",
        "    data_path.split('/')[-1].split('.')[0]: mean_squared_errors_list\n",
        "})"
      ],
      "metadata": {
        "id": "l49BzVLS1yGL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}